import streamlit as st
import ollama
from duckduckgo_search import DDGS

# --- Configuration de la Page ---
st.set_page_config(
    page_title="Chatbot avec Acc√®s Web",
    page_icon="üåê"
)

# --- Constantes ---
# Assurez-vous que ce mod√®le est bien t√©l√©charg√© via `ollama pull llama3`
OLLAMA_MODEL = 'gemma3n' 

# --- Fonctions Utilitaires ---

def search_the_web(query):
    """Effectue une recherche web et retourne les r√©sultats format√©s."""
    st.write(f"Recherche sur le web pour : '{query}'...")
    try:
        with DDGS() as ddgs:
            # max_results=4 pour ne pas surcharger le contexte
            results = list(ddgs.text(query, max_results=4))
            if not results:
                st.warning("Aucun r√©sultat trouv√© sur le web.")
                return "Aucune information trouv√©e sur le web."

            # Formatte les r√©sultats pour les injecter dans le prompt
            context = "Informations trouv√©es sur le web :\n\n"
            for i, result in enumerate(results):
                context += f"Source {i+1}: {result['title']}\n"
                context += f"Extrait: {result['body']}\n"
                context += f"URL: {result['href']}\n\n"
            
            return context
    except Exception as e:
        st.error(f"Erreur lors de la recherche web : {e}")
        return "Erreur lors de la r√©cup√©ration d'informations sur le web."

# --- Interface Streamlit ---

st.title("ü§ñ Chatbot Augment√© par le Web")
st.caption(f"Utilise le mod√®le `{OLLAMA_MODEL}` via Ollama")

st.info("Posez une question sur un √©v√©nement r√©cent, une personnalit√© ou un sujet d'actualit√©. L'assistant cherchera sur le web avant de vous r√©pondre.")

# Initialisation de l'historique de chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Affichage des messages de l'historique
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entr√©e utilisateur
if prompt := st.chat_input("Quelle est votre question ?"):
    # Ajouter le message de l'utilisateur √† l'historique et l'afficher
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Lancer le processus de r√©ponse de l'assistant
    with st.chat_message("assistant"):
        # 1. Effectuer la recherche web
        web_context = search_the_web(prompt)
        
        # 2. Construire le prompt augment√©
        system_prompt = f"""
        Tu es un assistant expert et utile. Ta t√¢che est de r√©pondre √† la question de l'utilisateur.
        Utilise les informations suivantes, extraites du web, pour construire ta r√©ponse.
        Cite tes sources en ajoutant les URLs √† la fin de ta r√©ponse si possible.
        Sois concis et direct.

        --- CONTEXTE WEB ---
        {web_context}
        --- FIN DU CONTEXTE ---
        """

        # 3. Envoyer le tout √† Ollama en mode streaming
        with st.spinner("L'assistant r√©fl√©chit..."):
            # On cr√©e un message syst√®me temporaire + le message utilisateur
            api_messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            # Fonction g√©n√©rateur pour extraire le contenu des chunks Ollama
            def response_generator():
                response_stream = ollama.chat(
                    model=OLLAMA_MODEL,
                    messages=api_messages,
                    stream=True
                )
                for chunk in response_stream:
                    if hasattr(chunk, 'message') and chunk.message.content:
                        yield chunk.message.content
                    elif isinstance(chunk, dict) and chunk.get('message', {}).get('content'):
                        yield chunk['message']['content']
            
            # Utilisation de st.write_stream avec le g√©n√©rateur corrig√©
            full_response = st.write_stream(response_generator())

    # 4. Ajouter la r√©ponse compl√®te de l'assistant √† l'historique
    st.session_state.messages.append({"role": "assistant", "content": full_response})